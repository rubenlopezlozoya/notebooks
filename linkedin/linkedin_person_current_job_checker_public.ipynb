{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# File paths for checkpoint and output\n",
    "PROGRESS_FILE = 'progress_checkpoint.csv'\n",
    "OUTPUT_FILE = 'output_with_job_check.csv'\n",
    "\n",
    "# Load the CSV into a DataFrame or resume from the last checkpoint\n",
    "if os.path.exists(PROGRESS_FILE):\n",
    "    df = pd.read_csv(PROGRESS_FILE)\n",
    "    print(\"Resuming from the last checkpoint.\")\n",
    "else:\n",
    "    df = pd.read_csv('linkedin_people.csv')\n",
    "    df['current_job_at_company'] = None  # Initialize the new column\n",
    "    print(\"Starting from the beginning.\")\n",
    "\n",
    "# Define the Zenrows API key and endpoint\n",
    "ZENROWS_API_KEY = '<API_KEY>'\n",
    "ZENROWS_ENDPOINT = 'https://api.zenrows.com/v1/'\n",
    "\n",
    "# Function to call Zenrows and parse the job positions with exponential retry\n",
    "def get_job_positions(index, linkedin_url, company_name):\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            params = {\n",
    "                'apikey': ZENROWS_API_KEY,\n",
    "                'url': linkedin_url,\n",
    "                'js_render': 'true',\n",
    "                'premium_proxy': 'true',\n",
    "            }\n",
    "            \n",
    "            response = requests.get(ZENROWS_ENDPOINT, params=params)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                job_positions = []\n",
    "\n",
    "                # Find grouped job experiences\n",
    "                experience_groups = soup.find_all('li', class_='experience-group')\n",
    "                for group in experience_groups:\n",
    "                    company_header_tag = group.find('h4', class_='experience-group-header__company')\n",
    "                    if company_header_tag:\n",
    "                        company_group_name = company_header_tag.text.strip()\n",
    "\n",
    "                        # Find individual job positions within the group\n",
    "                        grouped_jobs = group.find_all('li', class_='experience-group-position')\n",
    "                        for job in grouped_jobs:\n",
    "                            time_range_tag = job.find('span', class_='date-range text-color-text-secondary font-sans text-md leading-open font-regular')\n",
    "                            if time_range_tag:\n",
    "                                time_range = time_range_tag.text.strip()\n",
    "                                job_positions.append((company_group_name, time_range))\n",
    "\n",
    "                # Find standalone job positions outside of groups\n",
    "                standalone_jobs = soup.find_all('li', class_='profile-section-card relative flex w-full list-none py-1.5 pr-2 pl-1 experience-item')\n",
    "                for job in standalone_jobs:\n",
    "                    company_name_tag = job.find('span', class_='experience-item__subtitle')\n",
    "                    time_range_tag = job.find('span', class_='date-range text-color-text-secondary font-sans text-md leading-open font-regular')\n",
    "\n",
    "                    if company_name_tag and time_range_tag:\n",
    "                        company = company_name_tag.text.strip()\n",
    "                        time_range = time_range_tag.text.strip()\n",
    "                        job_positions.append((company, time_range))\n",
    "\n",
    "                # Check if any position matches the given company and contains \"Present\"\n",
    "                for company, time_range in job_positions:\n",
    "                    if (company.lower() == company_name.lower() or company.lower() in company_name.lower() or  company_name.lower() in company.lower()) and 'Present' in time_range:\n",
    "                        return index, True\n",
    "                \n",
    "                if not job_positions:\n",
    "                    return index, \"Not Found\"\n",
    "\n",
    "                return index, False\n",
    "            else:\n",
    "                print(f\"Attempt {attempt + 1} failed for {linkedin_url} with status code {response.status_code}. Retrying...\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Attempt {attempt + 1} failed for {linkedin_url} with error: {e}. Retrying...\")\n",
    "\n",
    "        # Exponential backoff: wait for 2^attempt seconds before retrying\n",
    "        time.sleep(2 ** attempt)\n",
    "\n",
    "    # If all retries fail, log the failure and return None\n",
    "    print(f\"Failed to retrieve data for {linkedin_url} after {max_retries} attempts.\")\n",
    "    return index, None\n",
    "\n",
    "# Process rows with non-empty LinkedIn URLs in parallel for the first 20 rows\n",
    "try:\n",
    "    with ThreadPoolExecutor(max_workers=40) as executor:\n",
    "        futures = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            if pd.notna(row['linkedin_url']) and (pd.isna(row['current_job_at_company']) or not row['current_job_at_company']):\n",
    "                futures.append(\n",
    "                    executor.submit(get_job_positions, index, row['linkedin_url'], row['company'])\n",
    "                )\n",
    "\n",
    "        # Use tqdm to show progress as tasks are completed\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing LinkedIn data\"):\n",
    "            index, result = future.result()\n",
    "            if result is not None:\n",
    "                df.at[index, 'current_job_at_company'] = result\n",
    "\n",
    "            # Save progress after each processed row\n",
    "            df.to_csv(PROGRESS_FILE, index=False)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Save the final result to the output file\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(\"Progress saved to the output file.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maquinillo-h_Y_WDD6-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
