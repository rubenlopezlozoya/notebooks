{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse, parse_qs, urlencode, quote\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to securely input sensitive information\n",
    "def secure_input(prompt):\n",
    "    import getpass\n",
    "    return getpass.getpass(prompt)\n",
    "\n",
    "# User Inputs\n",
    "linkedin_cookie = secure_input(\"Enter your LinkedIn Cookie (e.g., 'li_at=...; ...'): \")\n",
    "csrf_token = secure_input(\"Enter your CSRF Token (e.g., 'ajax:1234567890'): \")\n",
    "sales_navigator_url = input(\"Enter the LinkedIn Sales Navigator URL: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sales_navigator_url(url):\n",
    "    \"\"\"\n",
    "    Parses the Sales Navigator URL and extracts query parameters.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    # Flatten the query parameters (take first value for each key)\n",
    "    query_params = {k: v[0] for k, v in query_params.items()}\n",
    "    return query_params\n",
    "\n",
    "def build_api_url(base_api_url, query_params, start=0, count=25):\n",
    "    \"\"\"\n",
    "    Constructs the API URL with updated pagination parameters and correct encoding.\n",
    "    \"\"\"\n",
    "    query_params_copy = query_params.copy()\n",
    "    query_params_copy['start'] = str(start)\n",
    "    query_params_copy['count'] = str(count)\n",
    "    query_params_copy['q'] = 'searchQuery'\n",
    "    query_params_copy['decorationId'] = 'com.linkedin.sales.deco.desktop.searchv2.LeadSearchResult-14'\n",
    "    \n",
    "    # Ensure proper encoding of the 'query' parameter\n",
    "    if 'query' in query_params_copy:\n",
    "        # Encode special characters except for '(', ')', ':', ',', and '%'\n",
    "        # This prevents double encoding of already encoded characters like '%20'\n",
    "        query_encoded = quote(query_params_copy['query'], safe='(),:%')\n",
    "        query_params_copy['query'] = query_encoded\n",
    "    print(query_params_copy['query'])\n",
    "    # Reconstruct the query string\n",
    "    query_string = urlencode(query_params_copy, safe='(),:%')\n",
    "    return f\"{base_api_url}?{query_string}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the Sales Navigator URL to extract query parameters\n",
    "query_params = parse_sales_navigator_url(sales_navigator_url)\n",
    "\n",
    "# Define the base API endpoint\n",
    "base_api_url = 'https://www.linkedin.com/sales-api/salesApiLeadSearch'\n",
    "\n",
    "# Define headers\n",
    "headers = {\n",
    "    'Accept': '*/*',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'cookie': linkedin_cookie,\n",
    "    'csrf-token': csrf_token,\n",
    "    'x-restli-protocol-version': '2.0.0',\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "}\n",
    "\n",
    "# Pagination settings\n",
    "START = 0\n",
    "COUNT = 25\n",
    "MAX_RESULTS = 2000  # Total desired results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to hold all company elements\n",
    "all_leads = []\n",
    "\n",
    "# Calculate the number of iterations\n",
    "iterations = min(MAX_RESULTS // COUNT, 80)  # To ensure we don't exceed 2000\n",
    "\n",
    "print(f\"Starting scraping of {iterations * COUNT} records...\")\n",
    "\n",
    "for start in tqdm(range(START, iterations * COUNT, COUNT), desc=\"Scraping\"):\n",
    "    # Build the API URL for the current page\n",
    "    api_url = build_api_url(base_api_url, query_params, start=start, count=COUNT)\n",
    "\n",
    "    try:\n",
    "        # Make the GET request\n",
    "        response = requests.get(api_url, headers=headers)\n",
    "        \n",
    "        # Check for successful response\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            leads = data.get('elements', [])\n",
    "            all_leads.extend(leads)\n",
    "            print(f\"Fetched {len(leads)} leads at start={start}. Total fetched: {len(all_leads)}\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch data at start={start}. Status Code: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            # Optionally, implement retry logic or break\n",
    "            break\n",
    "        \n",
    "        # Optional: Sleep to respect rate limits\n",
    "        time.sleep(1)  # Sleep for 1 second between requests\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred at start={start}: {e}\")\n",
    "        break\n",
    "\n",
    "print(f\"Scraping completed. Total leads fetched: {len(all_leads)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Function to extract relevant fields from each company element\n",
    "def extract_lead_data(lead):\n",
    "    try:\n",
    "\n",
    "        linkedin_id = lead.get('entityUrn').split(':')[-1].split(',')[0][1:] if ':' in lead.get('entityUrn') else None\n",
    "\n",
    "        lead_data = {\n",
    "            'first_name': lead.get('lastName'),\n",
    "            'last_name': lead.get('firstName'),\n",
    "            'full_name': lead.get('fullName'),\n",
    "            'geo_region': lead.get('geoRegion'),\n",
    "            'summary': lead.get('summary'),\n",
    "            'company_name': lead.get('currentPositions')[0]['companyName'],\n",
    "            'company_id': lead.get('currentPositions')[0]['companyUrn'],\n",
    "            'current_job_title': lead.get('currentPositions')[0]['title'],\n",
    "            'entityUrn': lead.get('entityUrn'),\n",
    "            'linkedin_id': linkedin_id,\n",
    "            'objectUrn': lead.get('objectUrn')\n",
    "        }\n",
    "        \n",
    "        # Optional: Extract more fields as needed\n",
    "        \n",
    "        return lead_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data for a lead: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process all companies\n",
    "processed_leads = [extract_lead_data(lead) for lead in all_leads]\n",
    "# Remove None entries\n",
    "processed_leads = [lead for lead in processed_leads if lead is not None]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(processed_leads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('linkedin_scraped_people.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maquinillo-h_Y_WDD6-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
