{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import http\n",
    "import json\n",
    "\n",
    "\n",
    "# Function to convert product names to slugs\n",
    "def product_name_to_slug(product_name):\n",
    "\n",
    "    product_name = product_name.lower()\n",
    "    product_name = unidecode(product_name)\n",
    "    product_name = re.sub(r'\\s+', '-', product_name)\n",
    "    product_name = re.sub(r'[^a-z0-9-]', '', product_name)\n",
    "    return product_name\n",
    "\n",
    "def serper_name_to_g2(company_name):\n",
    "    conn = http.client.HTTPSConnection(\"google.serper.dev\")\n",
    "    payload = json.dumps({\n",
    "    \"q\": f\"{company_name} alternatives & competitors site:g2.com\"\n",
    "    })\n",
    "    headers = {\n",
    "    'X-API-KEY': '<API_KEY>',\n",
    "    'Content-Type': 'application/json'\n",
    "    }\n",
    "    conn.request(\"POST\", \"/search\", payload, headers)\n",
    "    res = conn.getresponse()\n",
    "    data = res.read().decode(\"utf-8\")\n",
    "    results = json.loads(data)\n",
    "\n",
    "    organic_results = results['organic']\n",
    "\n",
    "    g2_url = organic_results[0]['link'] if organic_results else None\n",
    "\n",
    "    return g2_url\n",
    "\n",
    "def scrape_g2_alternatives(g2_url):\n",
    "    apikey = '<API_KEY>'\n",
    "\n",
    "    params = {\n",
    "        'url': g2_url,\n",
    "        'apikey': apikey,\n",
    "        'js_render': 'true',\n",
    "        'wait_for': '.grid-x',\n",
    "        'premium_proxy': 'true',\n",
    "        'proxy_country': 'us',\n",
    "        'custom_headers': 'true',\n",
    "    }\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',\n",
    "    }\n",
    "    response = requests.get('https://api.zenrows.com/v1/', params=params, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    product_list = soup.find('ul', class_='mb-0 list--chevron')\n",
    "    products = [li.text for li in product_list.find_all('li')]\n",
    "\n",
    "    return products\n",
    "\n",
    "product_competitors = {}\n",
    "g2_urls = []\n",
    "\n",
    "def run():\n",
    "    company_df = pd.read_csv('./data/domains.csv')\n",
    "    company_names = company_df['domain'].to_list()\n",
    "\n",
    "    for idx, company_name in enumerate(company_names): \n",
    "\n",
    "        print(f\"Scraping company {company_name}\")\n",
    "\n",
    "        g2_url = serper_name_to_g2(company_name)\n",
    "        \n",
    "        if not g2_url or 'competitors/alternatives' not in g2_url:\n",
    "            continue\n",
    "\n",
    "        print(f\"Found G2 URL: {g2_url}\")\n",
    "        \n",
    "\n",
    "\n",
    "        attempts = 0\n",
    "        success = False\n",
    "\n",
    "        while attempts < 4 and not success:\n",
    "            try:\n",
    "                products = scrape_g2_alternatives(g2_url)\n",
    "                product_competitors[company_name] = products\n",
    "                success = True\n",
    "                print(f'Company: {company_name} competitors: {products}')\n",
    "            except Exception as e:\n",
    "                attempts += 1\n",
    "                wait_time = 2 ** attempts + random.random()  # Exponential backoff with jitter\n",
    "                print(f\"Attempt {attempts} failed: {e}. Retrying in {wait_time:.2f} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "        \n",
    "        if not success:\n",
    "            print(f\"Failed to scrape {company_name} after 3 attempts. Skipping to next company.\")\n",
    "            continue\n",
    "    \n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import maquinillo.settings\n",
    "import os\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import time\n",
    "import random\n",
    "import urllib\n",
    "import http.client\n",
    "import json\n",
    "import tldextract\n",
    "\n",
    "def get_base_domain(url):\n",
    "    extracted = tldextract.extract(url)\n",
    "    domain = f\"{extracted.domain}.{extracted.suffix}\"\n",
    "    return domain\n",
    "\n",
    "def sleep_random_time():\n",
    "    # Generate a random floating-point number between 0 and 2\n",
    "    random_time = 2 * random.random()\n",
    "\n",
    "    # Sleep for the random amount of time\n",
    "    time.sleep(random_time)\n",
    "\n",
    "def get_domain_from_name(name):\n",
    "    URL = f\"https://company.clearbit.com/v1/domains/find?name={name}\"\n",
    "\n",
    "    CLEARBIT_KEY = os.getenv(\"CLEARBIT_API_KEY\")\n",
    "\n",
    "    response = requests.get(URL, auth=HTTPBasicAuth(CLEARBIT_KEY, \"\"))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        return response_json[\"domain\"]\n",
    "\n",
    "    elif response.status_code == 404:\n",
    "        return None\n",
    "\n",
    "    elif response.status_code == 422:\n",
    "        print(\"Weird name:\", name)\n",
    "        return None\n",
    "\n",
    "    else:\n",
    "        print(\"Status code:\", response.status_code)\n",
    "        print(\"Body: \", response.json())\n",
    "        raise Exception(\"Weird scenario\")    \n",
    "    \n",
    "def serper_name_to_domain(company_name):\n",
    "    SERPER_KEY = os.getenv(\"SERPER_API_KEY\")\n",
    "    conn = http.client.HTTPSConnection(\"google.serper.dev\")\n",
    "    payload = json.dumps({\n",
    "    \"q\": f\"{company_name} company website\"\n",
    "    })\n",
    "    headers = {\n",
    "    'X-API-KEY': SERPER_KEY,\n",
    "    'Content-Type': 'application/json'\n",
    "    }\n",
    "    conn.request(\"POST\", \"/search\", payload, headers)\n",
    "    res = conn.getresponse()\n",
    "    data = res.read().decode(\"utf-8\")\n",
    "    results = json.loads(data)\n",
    "    website = results['organic'][0]['link']\n",
    "\n",
    "    if 'linkedin' in website:\n",
    "        return None\n",
    "\n",
    "    return get_base_domain(results['organic'][0]['link'])\n",
    "\n",
    "\n",
    "def name_to_domain(name):\n",
    "    print(f\"Processing {name}\")\n",
    "    \n",
    "    try:\n",
    "        clearbit_domain = get_domain_from_name(name)\n",
    "        if clearbit_domain:\n",
    "            domain = clearbit_domain\n",
    "        else:\n",
    "            domain = serper_name_to_domain(name)\n",
    "    except Exception as e:\n",
    "        return name\n",
    "    return domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(product_competitors)\n",
    "\n",
    "for key, values in product_competitors.items():\n",
    "    product_competitors[key] = [name_to_domain(value) for value in values]\n",
    "\n",
    "product_competitors_df = pd.DataFrame.from_dict(product_competitors, orient='index')\n",
    "product_competitors_df.to_csv('./data/annual_only_competitors.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_competitors = []\n",
    "\n",
    "for key, values in product_competitors.items():\n",
    "    all_competitors.extend([value for value in values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_competitors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maquinillo-h_Y_WDD6-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
