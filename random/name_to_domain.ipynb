{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import maquinillo.settings\n",
    "import os\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import time\n",
    "import random\n",
    "import urllib\n",
    "import http.client\n",
    "import json\n",
    "import tldextract\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "def get_base_domain(url):\n",
    "    extracted = tldextract.extract(url)\n",
    "    domain = f\"{extracted.domain}.{extracted.suffix}\"\n",
    "    return domain\n",
    "\n",
    "def sleep_random_time():\n",
    "    # Generate a random floating-point number between 0 and 2\n",
    "    random_time = 2 * random.random()\n",
    "\n",
    "    # Sleep for the random amount of time\n",
    "    time.sleep(random_time)\n",
    "\n",
    "def get_domain_from_name(name):\n",
    "    URL = f\"https://company.clearbit.com/v1/domains/find?name={name}\"\n",
    "\n",
    "    CLEARBIT_KEY = os.getenv(\"CLEARBIT_API_KEY\")\n",
    "\n",
    "    response = requests.get(URL, auth=HTTPBasicAuth(CLEARBIT_KEY, \"\"))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        return response_json[\"domain\"]\n",
    "\n",
    "    elif response.status_code == 404:\n",
    "        return None\n",
    "\n",
    "    elif response.status_code == 422:\n",
    "        print(\"Weird name:\", name)\n",
    "        return None\n",
    "\n",
    "    else:\n",
    "        print(\"Status code:\", response.status_code)\n",
    "        print(\"Body: \", response.json())\n",
    "        raise Exception(\"Weird scenario\")    \n",
    "    \n",
    "def serper_name_to_domain(company_name):\n",
    "    conn = http.client.HTTPSConnection(\"google.serper.dev\")\n",
    "    payload = json.dumps({\n",
    "    \"q\": f\"{company_name} company website\"\n",
    "    })\n",
    "    headers = {\n",
    "    'X-API-KEY': '<API_KEY>',\n",
    "    'Content-Type': 'application/json'\n",
    "    }\n",
    "    conn.request(\"POST\", \"/search\", payload, headers)\n",
    "    res = conn.getresponse()\n",
    "    data = res.read().decode(\"utf-8\")\n",
    "    results = json.loads(data)\n",
    "    website = results['organic'][0]['link']\n",
    "\n",
    "    if 'linkedin' in website:\n",
    "        return None\n",
    "\n",
    "    return get_base_domain(results['organic'][0]['link'])\n",
    "\n",
    "\n",
    "def enrich_company(row):\n",
    "    print(row['Company Name'])\n",
    "    try:\n",
    "        clearbit_domain =  get_domain_from_name(row['Company Name'])\n",
    "        if clearbit_domain:\n",
    "            row['domain_found'] = clearbit_domain\n",
    "        else:\n",
    "            row['domain_found'] = serper_name_to_domain(row['Company Name'])\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('subtitles.csv')\n",
    "\n",
    "# Load existing checkpoint if available\n",
    "checkpoint_file = 'checkpoint.csv'\n",
    "if os.path.exists(checkpoint_file):\n",
    "    df_enriched = pd.read_csv(checkpoint_file)\n",
    "    processed_rows = len(df_enriched)\n",
    "    print(f\"Resuming from row {processed_rows}...\")\n",
    "else:\n",
    "    # Create a Pandas DataFrame from the company data\n",
    "    df_enriched = pd.DataFrame(columns=df.columns)  # Create an empty DataFrame for enriched results\n",
    "    processed_rows = 0  # No rows processed yet\n",
    "\n",
    "# Rate limiting wrapper\n",
    "def rate_limited_enrich(row, requests_per_second):\n",
    "    try:\n",
    "        enriched_row = enrich_company(row)  # Call your actual function\n",
    "        time.sleep(1 / requests_per_second)  # Ensure rate limit is maintained\n",
    "        return enriched_row\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {row['company_name']}: {e}\")\n",
    "        return row  # Return the original row if there's an error, or handle as needed\n",
    "\n",
    "# Function to parallelize the processing with rate limiting and checkpointing\n",
    "def parallel_apply_with_checkpoint(df, func, requests_per_minute=600, num_workers=4, checkpoint_interval=500):\n",
    "    requests_per_second = requests_per_minute / 60  # Calculate how many requests per second\n",
    "    global df_enriched  # Use the global enriched DataFrame to store results\n",
    "    \n",
    "    # Only process the rows that haven't been processed yet\n",
    "    df_to_process = df.iloc[processed_rows:]\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        for idx, enriched_row in enumerate(executor.map(lambda row: rate_limited_enrich(row, requests_per_second), \n",
    "                                                        [row for _, row in df_to_process.iterrows()])):\n",
    "            # Append the result to the global enriched DataFrame\n",
    "            df_enriched = pd.concat([df_enriched, pd.DataFrame([enriched_row])], ignore_index=True)\n",
    "\n",
    "            # Checkpoint: Save progress every 'checkpoint_interval' rows\n",
    "            if (idx + 1) % checkpoint_interval == 0 or (idx + 1) == len(df_to_process):\n",
    "                df_enriched.to_csv(checkpoint_file, index=False)  # Save the checkpoint\n",
    "                print(f\"Checkpoint saved: {len(df_enriched)} rows processed.\")\n",
    "\n",
    "    return df_enriched\n",
    "\n",
    "# Apply the function in parallel while respecting the rate limit and checkpointing\n",
    "df_enriched = parallel_apply_with_checkpoint(df, enrich_company, requests_per_minute=600)\n",
    "\n",
    "# Final save to the output CSV file\n",
    "df_enriched.to_csv('buyers_enriched.csv', index=False)\n",
    "\n",
    "print(\"Processing complete and saved to domains_enriched.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maquinillo-h_Y_WDD6-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
