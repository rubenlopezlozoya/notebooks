{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Base URL and initial parameters for pagination\n",
    "base_url = 'https://www.lennysnewsletter.com/api/v1/archive'\n",
    "params = {\n",
    "    'sort': 'new',\n",
    "    'search': '',\n",
    "    'offset': 0,\n",
    "    'limit': 12,\n",
    "    'type': 'podcast',\n",
    "    'rss_episodes_only': 'true'\n",
    "}\n",
    "\n",
    "# Headers for the request (with a sample user-agent and necessary headers)\n",
    "headers = {\n",
    "    'accept': '*/*',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',\n",
    "    'referer': 'https://www.lennysnewsletter.com/podcast/archive?sort=new'\n",
    "}\n",
    "\n",
    "# List to store all results\n",
    "all_podcasts = []\n",
    "\n",
    "# Loop to handle pagination\n",
    "while True:\n",
    "    try:\n",
    "        # Make the request\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "        response.raise_for_status()  # Ensure the request was successful\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if data is a list\n",
    "        if not isinstance(data, list):\n",
    "            print(\"Unexpected response format, expected a list.\")\n",
    "            break\n",
    "\n",
    "        # Break the loop if no more data is returned\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        # Extract relevant data and store it\n",
    "        for item in data:\n",
    "            # Failsafe mechanism for transcripts\n",
    "            podcast_upload = item.get('podcastUpload')\n",
    "            transcript_url = 'N/A'  # Default value\n",
    "\n",
    "            if podcast_upload and isinstance(podcast_upload, dict):\n",
    "                transcription = podcast_upload.get('transcription', {})\n",
    "                if transcription and isinstance(transcription, dict):\n",
    "                    transcript_url = transcription.get('cdn_url', 'N/A')\n",
    "            \n",
    "            all_podcasts.append({\n",
    "                'Title': item.get('title', 'N/A'),\n",
    "                'URL': item.get('canonical_url', 'N/A'),\n",
    "                'Transcript CDN URL': transcript_url\n",
    "            })\n",
    "\n",
    "        # Update the offset for the next page\n",
    "        params['offset'] += params['limit']\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        break\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df = pd.DataFrame(all_podcasts)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Optionally save to a CSV\n",
    "# df.to_csv('podcasts_with_transcripts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Function to extract <p> tags and keep all text except what's inside <a> tags\n",
    "def extract_text_and_links(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the specific div containing the content\n",
    "        content_div = soup.find('div', class_='available-content')\n",
    "        if not content_div:\n",
    "            return []  # Return empty if the div is not found\n",
    "\n",
    "        # Find all <p> tags inside the \"available-content\" div\n",
    "        p_tags = content_div.find_all('p')\n",
    "        extracted_data = []\n",
    "\n",
    "        # Iterate over each <p> tag and clean the text\n",
    "        for p_tag in p_tags:\n",
    "            # Check if the <p> tag also contains <a> tags for extracting hrefs\n",
    "            # a_tag = p_tag.find('a', href=True)\n",
    "\n",
    "            # Remove all <a> tags from the <p> tag\n",
    "            for a_tag in p_tag.find_all('a'):\n",
    "                a_tag.decompose()\n",
    "            \n",
    "            # Get the cleaned text from the <p> tag\n",
    "            cleaned_text = p_tag.get_text(separator=' ').strip()\n",
    "\n",
    "            \n",
    "            if cleaned_text and a_tag:\n",
    "                href_value = a_tag['href']\n",
    "                extracted_data.append({'Span Value': cleaned_text, 'Link': href_value})\n",
    "\n",
    "        return extracted_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Sample DataFrame (assume 'df' is your DataFrame with columns 'Title' and 'URL')\n",
    "# df = pd.DataFrame({'Title': ['Podcast 1', 'Podcast 2'], 'URL': ['https://example.com/page1', 'https://example.com/page2']})\n",
    "\n",
    "# Iterate over the DataFrame and extract data with a random delay\n",
    "extracted_data_list = []\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing URLs\"):\n",
    "    url = row['URL']\n",
    "    podcast_title = row['Title']\n",
    "    \n",
    "    # Extract data from the current URL\n",
    "    references = extract_text_and_links(url)\n",
    "    extracted_data_list.append({\n",
    "        'Podcast Title': podcast_title,\n",
    "        'Podcast URL': url,\n",
    "        'References': references\n",
    "    })\n",
    "\n",
    "    # print(references)\n",
    "    \n",
    "    # Wait for a random amount of time between 1 and 5 seconds\n",
    "    time_to_wait = random.randint(1, 2)\n",
    "    time.sleep(time_to_wait)\n",
    "\n",
    "# Display the collected data\n",
    "for item in extracted_data_list:\n",
    "    print(item)\n",
    "\n",
    "# Optionally convert to DataFrame and save to CSV if needed\n",
    "# extracted_df = pd.DataFrame(extracted_data_list)\n",
    "# extracted_df.to_csv('extracted_podcast_references.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter references to only include those with \"amazon\" or \"goodreads\" in the link\n",
    "def filter_amazon_goodreads_references(references):\n",
    "    return [\n",
    "        item for item in references\n",
    "        if 'amazon' in item['Link'] or 'goodreads/book/show/' in item['Link']\n",
    "    ]\n",
    "\n",
    "# Iterate over each row in extracted_data_list and apply the filtering\n",
    "for item in extracted_data_list:\n",
    "    item['References Clean'] = filter_amazon_goodreads_references(item['References'])\n",
    "\n",
    "# Display the final filtered data\n",
    "for item in extracted_data_list:\n",
    "    print(item)\n",
    "\n",
    "# Optionally convert to DataFrame and save to CSV if needed\n",
    "# final_filtered_df = pd.DataFrame(extracted_data_list)\n",
    "# final_filtered_df.to_csv('final_filtered_podcast_references.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert extracted_data_list to a DataFrame with one row per reference\n",
    "flattened_data = []\n",
    "\n",
    "for item in extracted_data_list:\n",
    "    podcast_title = item['Podcast Title']\n",
    "    podcast_url = item['Podcast URL']\n",
    "    \n",
    "    for reference in item['References Clean']:\n",
    "        flattened_data.append({\n",
    "            'Episode Title': podcast_title,\n",
    "            'Episode URL': podcast_url,\n",
    "            'Reference Title': reference['Span Value'],\n",
    "            'Reference Link': reference['Link']\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the flattened data\n",
    "references_df = pd.DataFrame(flattened_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(references_df)\n",
    "\n",
    "# Optionally save to CSV\n",
    "# references_df.to_csv('podcast_references_expanded.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import json\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Define response models using Pydantic for book validation\n",
    "class BookValidationResponse(BaseModel):\n",
    "    is_real_book: bool\n",
    "\n",
    "class Book(BaseModel):\n",
    "    title: str\n",
    "    author: str\n",
    "\n",
    "# Function to call OpenAI for book extraction\n",
    "def extract_book_references(transcript_text):\n",
    "    prompt = (\n",
    "        \"This is a podcast transcript text. Your goal is to find any book references and store them as a JSON list of books. \"\n",
    "        \"Each book should have a 'title' and 'author'. If the author is not available, leave it blank. \"\n",
    "        f\"This is the text: {transcript_text}\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format=BookValidationResponse\n",
    "        )\n",
    "        return response.choices[0].message.parsed.books\n",
    "    except Exception as e:\n",
    "        return [{\"error\": str(e)}]\n",
    "\n",
    "# Function to call Serper API to retrieve the Goodreads URL\n",
    "def get_goodreads_url(title):\n",
    "    url = \"https://google.serper.dev/search\"\n",
    "    payload = json.dumps({\"q\": f\"{title} book url goodreads\"})\n",
    "    headers = {\n",
    "        'X-API-KEY': '<API_KEY>',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "        data = response.json()\n",
    "        \n",
    "        for result in data.get('organic', []):\n",
    "            if 'https://www.goodreads.com/book/show/' in result['link'] and title in result['title']:\n",
    "                return result['link']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching Goodreads URL: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to process each book entry\n",
    "def process_book_entry(args):\n",
    "    index, episode_title, episode_url, book = args\n",
    "    print(book)\n",
    "    title = book.title\n",
    "\n",
    "    # Get the Goodreads URL using Serper\n",
    "    book_url = get_goodreads_url(title)\n",
    "    \n",
    "    return {\n",
    "        'Index': index,  # Include index for later sorting\n",
    "        'Title': episode_title,\n",
    "        'URL': episode_url,\n",
    "        'Book Title': title,\n",
    "        'Book URL': book_url\n",
    "    }\n",
    "\n",
    "# Filter DataFrame to remove rows with empty or invalid 'Cleaned Transcript'\n",
    "df = df[~df['Cleaned Transcript'].isin([\"\", \"403 Forbidden - Access Denied\", \"Failed to fetch transcript\", \"Error in transcript format\"])]\n",
    "\n",
    "# Apply the function in parallel using ThreadPoolExecutor to extract book references\n",
    "def extract_books_in_parallel(df):\n",
    "    results_map = {}\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:\n",
    "        futures = {executor.submit(extract_book_references, row['Cleaned Transcript']): idx for idx, row in df.iterrows()}\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Extracting books\"):\n",
    "            idx = futures[future]\n",
    "            try:\n",
    "                results_map[idx] = future.result()\n",
    "            except Exception as e:\n",
    "                results_map[idx] = [{\"error\": str(e)}]\n",
    "    return results_map\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract books and store results in the DataFrame\n",
    "results_map = extract_books_in_parallel(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, result in results_map.items():\n",
    "    df.at[idx, 'Book References'] = result\n",
    "\n",
    "# Prepare arguments for parallel processing of book entries\n",
    "book_entries = []\n",
    "for index, row in df.iterrows():\n",
    "    episode_title = row['Title']\n",
    "    episode_url = row['URL']\n",
    "    book_references = row['Book References']  # Assuming this is a list of dictionaries\n",
    "    \n",
    "    for book in book_references:\n",
    "        book_entries.append((index, episode_title, episode_url, book))\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel processing of book verification and URL retrieval\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:\n",
    "    results = list(tqdm(executor.map(process_book_entry, book_entries), total=len(book_entries), desc=\"Processing books\"))\n",
    "\n",
    "# Convert results to DataFrame and sort by 'Index' to ensure mapping integrity\n",
    "verified_books_df = pd.DataFrame(results)\n",
    "verified_books_df.sort_values(by='Index', inplace=True)\n",
    "verified_books_df.drop(columns=['Index'], inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "verified_books_df.to_csv('verified_books.csv', index=False)\n",
    "\n",
    "print(\"CSV file 'verified_books.csv' has been created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where the book is not real, the author is empty, or the book title is empty\n",
    "filtered_books_df = verified_books_df[\n",
    "    # (verified_books_df['Is Real Book']) &\n",
    "    (verified_books_df['Book Title'].str.strip() != '')\n",
    "]\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_books_df.to_csv('filtered_verified_books.csv', index=False)\n",
    "\n",
    "print(\"CSV file 'filtered_verified_books.csv' has been created with only valid book entries.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maquinillo-h_Y_WDD6-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
